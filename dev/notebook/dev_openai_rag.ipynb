{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bank AI Assistant - OpenAI RAG System\n",
        "\n",
        "This notebook demonstrates a Retrieval-Augmented Generation (RAG) system using OpenAI's embeddings and GPT models for answering questions about Bank of America's Online Banking Service Agreement.\n",
        "\n",
        "## Features:\n",
        "- OpenAI text-embedding-3-small for high-quality embeddings\n",
        "- GPT-3.5-turbo for natural language answer generation\n",
        "- Persistent index storage for fast subsequent runs\n",
        "- Batch processing for efficient API usage\n",
        "- Similarity search with cosine similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dependencies and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai scikit-learn langchain pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pypdf import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import openai\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pdf(path):\n",
        "    \"\"\"Load and extract text from PDF file\"\"\"\n",
        "    reader = PdfReader(path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def setup_openai(api_key=None):\n",
        "    \"\"\"Setup OpenAI API key\"\"\"\n",
        "    if api_key:\n",
        "        openai.api_key = api_key\n",
        "    elif os.getenv(\"OPENAI_API_KEY\"):\n",
        "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    else:\n",
        "        print(\"Please set your OpenAI API key:\")\n",
        "        print(\"export OPENAI_API_KEY='your-api-key-here'\")\n",
        "        return False\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. OpenAI RAG Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OpenAIRAG:\n",
        "    def __init__(self, model=\"text-embedding-3-small\"):\n",
        "        self.model = model\n",
        "        self.embeddings = []\n",
        "        self.chunks = []\n",
        "    \n",
        "    def load_document(self, pdf_path):\n",
        "        \"\"\"Load and chunk the PDF document\"\"\"\n",
        "        print(\"Loading PDF document...\")\n",
        "        document_text = load_pdf(pdf_path)\n",
        "        print(f\"Document length (chars): {len(document_text)}\")\n",
        "        \n",
        "        # Chunk the text\n",
        "        print(\"Chunking text...\")\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=800,\n",
        "            chunk_overlap=100,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        "        )\n",
        "        \n",
        "        self.chunks = splitter.split_text(document_text)\n",
        "        print(f\"Total chunks created: {len(self.chunks)}\")\n",
        "        return self.chunks\n",
        "    \n",
        "    def get_embeddings(self, texts, batch_size=100):\n",
        "        \"\"\"Get embeddings from OpenAI API with batching\"\"\"\n",
        "        print(f\"Getting embeddings for {len(texts)} chunks...\")\n",
        "        all_embeddings = []\n",
        "        \n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            print(f\"Processing batch {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}\")\n",
        "            \n",
        "            try:\n",
        "                response = openai.embeddings.create(\n",
        "                    model=self.model,\n",
        "                    input=batch\n",
        "                )\n",
        "                \n",
        "                batch_embeddings = [data.embedding for data in response.data]\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "                \n",
        "                # Small delay to respect rate limits\n",
        "                time.sleep(0.1)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error getting embeddings for batch {i//batch_size + 1}: {e}\")\n",
        "                # Retry with smaller batch\n",
        "                if batch_size > 10:\n",
        "                    print(\"Retrying with smaller batch size...\")\n",
        "                    return self.get_embeddings(texts, batch_size // 2)\n",
        "                else:\n",
        "                    raise e\n",
        "        \n",
        "        return all_embeddings\n",
        "    \n",
        "    def build_index(self):\n",
        "        \"\"\"Build the search index using OpenAI embeddings\"\"\"\n",
        "        print(\"Building search index with OpenAI embeddings...\")\n",
        "        \n",
        "        # Get embeddings for all chunks\n",
        "        self.embeddings = self.get_embeddings(self.chunks)\n",
        "        \n",
        "        # Convert to numpy array for similarity search\n",
        "        self.embeddings = np.array(self.embeddings)\n",
        "        print(f\"Embeddings shape: {self.embeddings.shape}\")\n",
        "        print(\"Index built successfully!\")\n",
        "    \n",
        "    def search(self, query, top_k=3):\n",
        "        \"\"\"Search for relevant chunks using cosine similarity\"\"\"\n",
        "        # Get embedding for the query\n",
        "        query_embedding = self.get_embeddings([query])[0]\n",
        "        query_vector = np.array(query_embedding).reshape(1, -1)\n",
        "        \n",
        "        # Calculate cosine similarities\n",
        "        similarities = cosine_similarity(query_vector, self.embeddings).flatten()\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "        \n",
        "        results = []\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            results.append({\n",
        "                'chunk': self.chunks[idx],\n",
        "                'similarity': similarities[idx],\n",
        "                'rank': i + 1\n",
        "            })\n",
        "        return results\n",
        "    \n",
        "    def answer_question(self, question, top_k=3):\n",
        "        \"\"\"Answer a question using retrieved context\"\"\"\n",
        "        # Search for relevant chunks\n",
        "        results = self.search(question, top_k)\n",
        "        \n",
        "        # Combine context\n",
        "        context = \"\\n\\n\".join([r['chunk'] for r in results])\n",
        "        \n",
        "        # Generate answer using OpenAI\n",
        "        try:\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about Bank of America's Online Banking Service Agreement. Use only the provided context to answer questions. If the answer is not in the context, say 'I don't have enough information to answer that question.'\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
        "                ],\n",
        "                max_tokens=300,\n",
        "                temperature=0.1\n",
        "            )\n",
        "            \n",
        "            answer = response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating answer: {e}\")\n",
        "            answer = f\"Based on the Bank of America Online Banking Service Agreement:\\n\\n{context[:1000]}...\"\n",
        "        \n",
        "        return answer, results\n",
        "    \n",
        "    def save_index(self, filepath=\"openai_rag_index.pkl\"):\n",
        "        \"\"\"Save the built index for future use\"\"\"\n",
        "        index_data = {\n",
        "            'chunks': self.chunks,\n",
        "            'embeddings': self.embeddings,\n",
        "            'model': self.model\n",
        "        }\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(index_data, f)\n",
        "        print(f\"Index saved to {filepath}\")\n",
        "    \n",
        "    def load_index(self, filepath=\"openai_rag_index.pkl\"):\n",
        "        \"\"\"Load a previously built index\"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                index_data = pickle.load(f)\n",
        "            self.chunks = index_data['chunks']\n",
        "            self.embeddings = index_data['embeddings']\n",
        "            self.model = index_data['model']\n",
        "            print(f\"Index loaded from {filepath}\")\n",
        "            return True\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
