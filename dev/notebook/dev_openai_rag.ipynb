{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bank AI Assistant - OpenAI RAG System\n",
        "\n",
        "This notebook demonstrates a Retrieval-Augmented Generation (RAG) system using OpenAI's embeddings and GPT models for answering questions about Bank of America's Online Banking Service Agreement.\n",
        "\n",
        "## Features:\n",
        "- OpenAI text-embedding-3-small for high-quality embeddings\n",
        "- GPT-3.5-turbo for natural language answer generation\n",
        "- Persistent index storage for fast subsequent runs\n",
        "- Batch processing for efficient API usage\n",
        "- Similarity search with cosine similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dependencies and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (1.106.1)\n",
            "Requirement already satisfied: scikit-learn in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (1.6.1)\n",
            "Requirement already satisfied: langchain in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (0.2.1)\n",
            "Requirement already satisfied: pypdf in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (4.2.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain) (3.12.15)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain) (0.2.43)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain) (0.1.147)\n",
            "Requirement already satisfied: requests<3,>=2 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.20.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (24.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /Volumes/nvme1TB_SSD/IdeaProjects/Bank-AI-Assistant-RAG/rag_env/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install openai scikit-learn langchain pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pypdf import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import openai\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pdf(path):\n",
        "    \"\"\"Load and extract text from PDF file\"\"\"\n",
        "    reader = PdfReader(path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def setup_openai(api_key=None):\n",
        "    \"\"\"Setup OpenAI API key\"\"\"\n",
        "    if api_key:\n",
        "        openai.api_key = api_key\n",
        "    elif os.getenv(\"OPENAI_API_KEY\"):\n",
        "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    else:\n",
        "        print(\"Please set your OpenAI API key:\")\n",
        "        print(\"export OPENAI_API_KEY='your-api-key-here'\")\n",
        "        return False\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Loading PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document length (chars): 124022\n"
          ]
        }
      ],
      "source": [
        "pdf_path = \"../../dataset/Bank_of_America_Online Banking_Service Agreement.pdf\"\n",
        "document_text = load_pdf(pdf_path)\n",
        "print(\"Document length (chars):\", len(document_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Chunking the Text\n",
        "We'll split the document into manageable chunks for embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks created: 173\n",
            "First chunk preview: Online Banking\n",
            "Online Banking Service Agreement\n",
            "‹‹  Go to Online Banking Overview\n",
            "Bank of America Online Banking Service Agreement\n",
            "Effective Date: July 21, 2025\n",
            "Table of Contents: Hide all Topics\n",
            "1. General Description of Bank of America Online Banking Service Agreement (this \"Agreement\")\n",
            "Introducti\n"
          ]
        }
      ],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        ")\n",
        "chunks = splitter.split_text(document_text)\n",
        "print(f\"Total chunks created: {len(chunks)}\")\n",
        "print(\"First chunk preview:\", chunks[0][:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. OpenAI Embedding Test\n",
        "Getting an embedding for a single chunk to make sure our API key works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding length: 1536\n"
          ]
        }
      ],
      "source": [
        "setup_openai()\n",
        "try:\n",
        "    response = openai.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=[chunks[0]]\n",
        "    )\n",
        "    print(\"Embedding length:\", len(response.data[0].embedding))\n",
        "except Exception as e:\n",
        "    print(\"Error getting embedding:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Batch Embedding Function\n",
        "Now, let's write a function to get embeddings for all chunks, batching to avoid rate limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embeddings(texts, model=\"text-embedding-3-small\", batch_size=100):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        print(f\"Batch {i//batch_size + 1}\")\n",
        "        try:\n",
        "            response = openai.embeddings.create(\n",
        "                model=model,\n",
        "                input=batch\n",
        "            )\n",
        "            batch_embeddings = [data.embedding for data in response.data]\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "            time.sleep(0.1)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
        "            if batch_size > 10:\n",
        "                print(\"Retrying with smaller batch size...\")\n",
        "                return get_embeddings(texts, model, batch_size // 2)\n",
        "            else:\n",
        "                raise e\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Build Embedding Index\n",
        "build the embedding index for all chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1\n",
            "Batch 2\n",
            "Embeddings shape: (173, 1536)\n"
          ]
        }
      ],
      "source": [
        "embeddings = get_embeddings(chunks)\n",
        "embeddings = np.array(embeddings)\n",
        "print(\"Embeddings shape:\", embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cosine Similarity Search\n",
        "function to search for the most relevant chunks using cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search(query, chunks, embeddings, model=\"text-embedding-3-small\", top_k=3):\n",
        "    query_embedding = get_embeddings([query], model=model)[0]\n",
        "    query_vector = np.array(query_embedding).reshape(1, -1)\n",
        "    similarities = cosine_similarity(query_vector, embeddings).flatten()\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    results = []\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        results.append({\n",
        "            'chunk': chunks[idx],\n",
        "            'similarity': similarities[idx],\n",
        "            'rank': i + 1\n",
        "        })\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Trying a Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1\n",
            "Rank 1 (score=0.755):\n",
            "your initial enrollment in the Service, or for certain recipients. The minimum transfer amount for any single Zelle  transfer is $1.00. There are no\n",
            "receiving limits for Zelle  transfers.\n",
            "Zelle  trans\n",
            "\n",
            "Rank 2 (score=0.709):\n",
            "transactions\n",
            "$45,000 / 30\n",
            "transactions$60,000 /120\n",
            "transactionsZelle  enrollment tenure 16-30 days $4,000 / 20\n",
            "transactions\n",
            "Zelle  enrollment tenure 31-60 days $8,000 / 20\n",
            "transactions\n",
            "Zelle  enrollme\n",
            "\n",
            "Rank 3 (score=0.648):\n",
            "limits will be lower for the first 60 days of your initial Zelle enrollment with your customer profile (User ID). W e also limit the amount you can send\n",
            "to certain recipients on a daily basis. W e lim\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = search(\"What is the Zelle transfer limit for new users?\", chunks, embeddings)\n",
        "for r in results:\n",
        "    print(f\"Rank {r['rank']} (score={r['similarity']:.3f}):\\n{r['chunk'][:200]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Generate Answer with GPT\n",
        "GPT-3.5-turbo to generate an answer using the top chunks as context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_question(question, chunks, embeddings, top_k=3):\n",
        "    results = search(question, chunks, embeddings, top_k=top_k)\n",
        "    context = \"\\n\\n\".join([r['chunk'] for r in results])\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about Bank of America's Online Banking Service Agreement. Use only the provided context to answer questions. If the answer is not in the context, say 'I don't have enough information to answer that question.'\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
        "            ],\n",
        "            max_tokens=300,\n",
        "            temperature=0.1\n",
        "        )\n",
        "        answer = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer: {e}\")\n",
        "        answer = f\"Based on the Bank of America Online Banking Service Agreement:\\n\\n{context[:1000]}...\"\n",
        "    return answer, results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Full RAG QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1\n",
            "Answer: You can cancel a scheduled bill payment by following the instructions on Bank of America's website. The cancel feature can be found in the payment activity section. Additionally, you may also request to cancel a future scheduled or recurring transfer by calling Bank of America at 800.432.1000 for consumer accounts and 866.758.5972 for small business accounts.\n"
          ]
        }
      ],
      "source": [
        "question = \"How do I cancel a scheduled bill payment?\"\n",
        "answer, context_results = answer_question(question, chunks, embeddings)\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Save/Load Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_index(filepath, chunks, embeddings, model):\n",
        "    index_data = {\n",
        "        'chunks': chunks,\n",
        "        'embeddings': embeddings,\n",
        "        'model': model\n",
        "    }\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(index_data, f)\n",
        "    print(f\"Index saved to {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_index(filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        with open(filepath, 'rb') as f:\n",
        "            index_data = pickle.load(f)\n",
        "        print(f\"Index loaded from {filepath}\")\n",
        "        return index_data['chunks'], index_data['embeddings'], index_data['model']\n",
        "    return None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index saved to ../data/openai_rag_index.pkl\n"
          ]
        }
      ],
      "source": [
        "save_index(\"../data/openai_rag_index.pkl\", chunks, embeddings, \"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index loaded from ../data/openai_rag_index.pkl\n",
            "Index loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "chunks, embeddings, model = load_index(\"../data/openai_rag_index.pkl\")\n",
        "if chunks is not None:\n",
        "    print(\"Index loaded successfully!\")\n",
        "else:\n",
        "    print(\"No index found, please build it first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
